{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TF = \\frac{{\\text{{number of instances of a word in a document}}}}{{\\text{{total number of words in a document}}}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "IDF = log \\frac{{\\text{{total number of documents (N) in text corpus D}}}}{{\\text{{number of documents containg w}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>id17718</td>\n",
       "      <td>I could have fancied, while I looked at it, th...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>id08973</td>\n",
       "      <td>The lids clenched themselves together as if in...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>id05267</td>\n",
       "      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>id17513</td>\n",
       "      <td>For an item of news like this, it strikes us i...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>id00393</td>\n",
       "      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19579 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author\n",
       "0      id26305  This process, however, afforded me no means of...    EAP\n",
       "1      id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2      id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3      id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4      id12958  Finding nothing else, not even gold, the Super...    HPL\n",
       "...        ...                                                ...    ...\n",
       "19574  id17718  I could have fancied, while I looked at it, th...    EAP\n",
       "19575  id08973  The lids clenched themselves together as if in...    EAP\n",
       "19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP\n",
       "19577  id17513  For an item of news like this, it strikes us i...    EAP\n",
       "19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL\n",
       "\n",
       "[19579 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values\n",
    "x_train,x_val, y_train,y_val = train_test_split(df['text'],y,stratify=y,test_size=0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`, `TfidfVectorizer`, and `TfidfTransformer` are all tools commonly used in natural language processing (NLP) pipelines, but they serve slightly different purposes. Here's how they can be used together or standalone:\n",
    "\n",
    "1. **CountVectorizer**:\n",
    "   - Standalone: CountVectorizer is used to convert a collection of text documents into a matrix of token counts, where each row represents a document and each column represents a word, with each cell indicating the count of the corresponding word in the document.\n",
    "   - Together: CountVectorizer can be used as the initial step in preprocessing text data, followed by using either `TfidfTransformer` or `TfidfVectorizer` to convert the count matrix into a TF-IDF (Term Frequency-Inverse Document Frequency) weighted representation.\n",
    "\n",
    "2. **TfidfVectorizer**:\n",
    "   - Standalone: TfidfVectorizer combines the functionality of CountVectorizer and TfidfTransformer into a single step. It first tokenizes the text and counts occurrences of each token like CountVectorizer, then it transforms the count matrix into a TF-IDF representation like TfidfTransformer.\n",
    "   - Together: TfidfVectorizer can be used as a replacement for CountVectorizer and TfidfTransformer together, simplifying the preprocessing pipeline.\n",
    "\n",
    "3. **TfidfTransformer**:\n",
    "   - Standalone: TfidfTransformer is used to transform a count matrix (obtained from CountVectorizer or similar methods) into a TF-IDF representation. It weighs the importance of each word in the document relative to the entire corpus.\n",
    "   - Together: TfidfTransformer is typically used after CountVectorizer to convert the count matrix into a TF-IDF representation.\n",
    "\n",
    "In summary, here's how they can be used:\n",
    "- **CountVectorizer**: Converts text into a count matrix.\n",
    "- **TfidfVectorizer**: Combines CountVectorizer and TfidfTransformer into one step, producing a TF-IDF representation directly from raw text.\n",
    "- **TfidfTransformer**: Converts a count matrix into a TF-IDF representation, typically used after CountVectorizer. \n",
    "\n",
    "You can choose to use them standalone or in combination based on the specific requirements and complexity of your NLP task. If you need a simple preprocessing pipeline, you can use `TfidfVectorizer`. If you need more control over the preprocessing steps, you can use `CountVectorizer` followed by `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfb = TfidfVectorizer(min_df=3,max_features=None,strip_accents='unicode',analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1,5),use_idf=True,smooth_idf=True,sublinear_tf=True,stop_words='english')\n",
    "\n",
    "tfb.fit(list(x_train)+list(x_val))\n",
    "x_train_tfb = tfb.transform(x_train)\n",
    "x_val_tfb = tfb.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_result(y_pred_prob,y_pred,x_val,y_val):\n",
    "    print(y_val)\n",
    "    mse_loss = mean_squared_error(y_pred,y_val)\n",
    "    print(f'MSE loss: {mse_loss}')\n",
    "    print(classification_report(y_val, y_pred, target_names=['0', '1','2']))\n",
    "    table_submis = pd.DataFrame(y_pred_prob, columns=['EAP', 'MWS', 'HPL'])\n",
    "    table_submis['id'] = df.loc[x_val.index,'id'].values\n",
    "    table_submis = table_submis[['id','EAP','HPL','MWS']]\n",
    "\n",
    "    result_df = pd.DataFrame({'Index': df.loc[x_val.index,'id'],'text':x_val, 'Predicted Label': lbl_enc.inverse_transform(y_pred), 'Actual Label': lbl_enc.inverse_transform(y_val)})\n",
    "    return table_submis, result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjain/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "### Logistic Regression\n",
    "\n",
    "lg = LogisticRegression()\n",
    "lg.fit(x_train_tfb,y_train)\n",
    "y_pred = lg.predict(x_val_tfb)\n",
    "y_pred_proba = lg.predict_proba(x_val_tfb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 ... 0 2 1]\n",
      "MSE loss: 0.4703779366700715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80       790\n",
      "           1       0.83      0.76      0.79       564\n",
      "           2       0.82      0.76      0.79       604\n",
      "\n",
      "    accuracy                           0.80      1958\n",
      "   macro avg       0.81      0.79      0.80      1958\n",
      "weighted avg       0.80      0.80      0.80      1958\n",
      "\n",
      "           id       EAP       HPL       MWS\n",
      "0     id24239  0.020640  0.959664  0.019697\n",
      "1     id22765  0.123976  0.845248  0.030777\n",
      "2     id06298  0.767866  0.158755  0.073379\n",
      "3     id10325  0.262326  0.036366  0.701308\n",
      "4     id26532  0.752308  0.176806  0.070886\n",
      "...       ...       ...       ...       ...\n",
      "1953  id21175  0.921809  0.044164  0.034027\n",
      "1954  id07913  0.452738  0.420180  0.127081\n",
      "1955  id04754  0.546432  0.078788  0.374780\n",
      "1956  id02574  0.841114  0.030806  0.128080\n",
      "1957  id22770  0.194371  0.165192  0.640436\n",
      "\n",
      "[1958 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "table_submis, result_df = final_result(y_pred_proba,y_pred,x_val,y_val)\n",
    "print(table_submis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Index                                               text  \\\n",
      "5144   id24239  Adrian had the superiority in learning and elo...   \n",
      "8125   id22765  Let me hear the sweet tones of your beloved vo...   \n",
      "13261  id06298  They are all, however, fitting tapestry for a ...   \n",
      "6631   id10325  Its two general directions, as I have said, we...   \n",
      "584    id26532  After marriage, however, this gentleman neglec...   \n",
      "...        ...                                                ...   \n",
      "9712   id21175  But it has been suggested that the corpse disc...   \n",
      "10254  id07913                              \"How can I move thee?   \n",
      "18562  id04754  \"Moissart and Voissart\" I repeated, thoughtful...   \n",
      "18720  id02574                             Yet why do I say this?   \n",
      "6467   id22770  So the White Ship sailed on past the walls of ...   \n",
      "\n",
      "      Predicted Label Actual Label  \n",
      "5144              MWS          MWS  \n",
      "8125              MWS          MWS  \n",
      "13261             EAP          EAP  \n",
      "6631              HPL          EAP  \n",
      "584               EAP          EAP  \n",
      "...               ...          ...  \n",
      "9712              EAP          EAP  \n",
      "10254             EAP          MWS  \n",
      "18562             EAP          EAP  \n",
      "18720             EAP          MWS  \n",
      "6467              HPL          HPL  \n",
      "\n",
      "[1958 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xb = xgb.XGBClassifier()\n",
    "\n",
    "xb.fit(x_train_tfb,y_train)\n",
    "y_pred_prob = xb.predict_proba(x_val_tfb)\n",
    "y_pred = xb.predict(x_val_tfb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 ... 0 2 1]\n",
      "MSE loss: 0.7563840653728294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.87      0.71       790\n",
      "           1       0.75      0.56      0.64       564\n",
      "           2       0.79      0.53      0.64       604\n",
      "\n",
      "    accuracy                           0.68      1958\n",
      "   macro avg       0.71      0.65      0.66      1958\n",
      "weighted avg       0.70      0.68      0.67      1958\n",
      "\n",
      "           id       EAP       HPL       MWS\n",
      "0     id24239  0.020640  0.959664  0.019697\n",
      "1     id22765  0.123976  0.845248  0.030777\n",
      "2     id06298  0.767866  0.158755  0.073379\n",
      "3     id10325  0.262326  0.036366  0.701308\n",
      "4     id26532  0.752308  0.176806  0.070886\n",
      "...       ...       ...       ...       ...\n",
      "1953  id21175  0.921809  0.044164  0.034027\n",
      "1954  id07913  0.452738  0.420180  0.127081\n",
      "1955  id04754  0.546432  0.078788  0.374780\n",
      "1956  id02574  0.841114  0.030806  0.128080\n",
      "1957  id22770  0.194371  0.165192  0.640436\n",
      "\n",
      "[1958 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "table_submis, result_df = final_result(y_pred_proba,y_pred,x_val,y_val)\n",
    "print(table_submis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Index                                               text  \\\n",
      "5144   id24239  Adrian had the superiority in learning and elo...   \n",
      "8125   id22765  Let me hear the sweet tones of your beloved vo...   \n",
      "13261  id06298  They are all, however, fitting tapestry for a ...   \n",
      "6631   id10325  Its two general directions, as I have said, we...   \n",
      "584    id26532  After marriage, however, this gentleman neglec...   \n",
      "...        ...                                                ...   \n",
      "9712   id21175  But it has been suggested that the corpse disc...   \n",
      "10254  id07913                              \"How can I move thee?   \n",
      "18562  id04754  \"Moissart and Voissart\" I repeated, thoughtful...   \n",
      "18720  id02574                             Yet why do I say this?   \n",
      "6467   id22770  So the White Ship sailed on past the walls of ...   \n",
      "\n",
      "      Predicted Label Actual Label  \n",
      "5144              MWS          MWS  \n",
      "8125              MWS          MWS  \n",
      "13261             EAP          EAP  \n",
      "6631              HPL          EAP  \n",
      "584               EAP          EAP  \n",
      "...               ...          ...  \n",
      "9712              EAP          EAP  \n",
      "10254             EAP          MWS  \n",
      "18562             HPL          EAP  \n",
      "18720             EAP          MWS  \n",
      "6467              HPL          HPL  \n",
      "\n",
      "[1958 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unseen data\n",
    "x_test = test['text']\n",
    "x_test_tfb = tfb.transform(test['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lg.predict(x_test_tfb)\n",
    "y_pred_proba = lg.predict_proba(x_test_tfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8392, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_final_result(y_pred_prob,y_pred,x_test):\n",
    "    table_submis = pd.DataFrame(y_pred_prob, columns=['EAP', 'MWS', 'HPL'])\n",
    "    table_submis['id'] = test.loc[x_test.index,'id'].values\n",
    "    table_submis = table_submis[['id','EAP','HPL','MWS']]\n",
    "\n",
    "    result_df = pd.DataFrame({'Index': test.loc[x_test.index,'id'],'text':x_test, 'Predicted Label': lbl_enc.inverse_transform(y_pred)})\n",
    "\n",
    "    return table_submis, result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_submis, result_df = unseen_final_result(y_pred_proba,y_pred,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>text</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8387</th>\n",
       "      <td>id11749</td>\n",
       "      <td>All this is now the fitter for my purpose.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8388</th>\n",
       "      <td>id10526</td>\n",
       "      <td>I fixed myself on a wide solitude.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8389</th>\n",
       "      <td>id13477</td>\n",
       "      <td>It is easily understood that what might improv...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8390</th>\n",
       "      <td>id13761</td>\n",
       "      <td>Be this as it may, I now began to feel the ins...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8391</th>\n",
       "      <td>id04282</td>\n",
       "      <td>Long winded, statistical, and drearily genealo...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8392 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Index                                               text  \\\n",
       "0     id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1     id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2     id00134  And when they had broken down the frail door t...   \n",
       "3     id27757  While I was thinking how I should possibly man...   \n",
       "4     id04081  I am not sure to what limit his knowledge may ...   \n",
       "...       ...                                                ...   \n",
       "8387  id11749         All this is now the fitter for my purpose.   \n",
       "8388  id10526                 I fixed myself on a wide solitude.   \n",
       "8389  id13477  It is easily understood that what might improv...   \n",
       "8390  id13761  Be this as it may, I now began to feel the ins...   \n",
       "8391  id04282  Long winded, statistical, and drearily genealo...   \n",
       "\n",
       "     Predicted Label  \n",
       "0                MWS  \n",
       "1                EAP  \n",
       "2                EAP  \n",
       "3                EAP  \n",
       "4                EAP  \n",
       "...              ...  \n",
       "8387             MWS  \n",
       "8388             MWS  \n",
       "8389             EAP  \n",
       "8390             MWS  \n",
       "8391             HPL  \n",
       "\n",
       "[8392 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anujjain/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "# Misc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/train.csv')\n",
    "test = pd.read_csv('Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = LabelEncoder()\n",
    "df['author_encoder'] = lbl_enc.fit_transform(df['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_encoder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>id17718</td>\n",
       "      <td>I could have fancied, while I looked at it, th...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>id08973</td>\n",
       "      <td>The lids clenched themselves together as if in...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>id05267</td>\n",
       "      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>id17513</td>\n",
       "      <td>For an item of news like this, it strikes us i...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>id00393</td>\n",
       "      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19579 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author  \\\n",
       "0      id26305  This process, however, afforded me no means of...    EAP   \n",
       "1      id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2      id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3      id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4      id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "...        ...                                                ...    ...   \n",
       "19574  id17718  I could have fancied, while I looked at it, th...    EAP   \n",
       "19575  id08973  The lids clenched themselves together as if in...    EAP   \n",
       "19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP   \n",
       "19577  id17513  For an item of news like this, it strikes us i...    EAP   \n",
       "19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL   \n",
       "\n",
       "       author_encoder  \n",
       "0                   0  \n",
       "1                   1  \n",
       "2                   0  \n",
       "3                   2  \n",
       "4                   1  \n",
       "...               ...  \n",
       "19574               0  \n",
       "19575               0  \n",
       "19576               0  \n",
       "19577               0  \n",
       "19578               1  \n",
       "\n",
       "[19579 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "Model = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(Model)\n",
    "bert_model = BertModel.from_pretrained(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens=[]\n",
    "\n",
    "for txt in df.text:\n",
    "    tokens = tokenizer.encode(txt,max_length=512,truncation=True)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Token count')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of review lengths \n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Most of the reviews seem to contain less than 120 tokens, but we’ll be on the safe side and choose a maximum length of 160.\n",
    "MAX_LEN = 160\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "class auth_dataset(Dataset):\n",
    "    def __init__(self,txt,authors,tokenizer,max_len):\n",
    "        self.txt = txt\n",
    "        self.authors = authors\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.txt)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        txt = str(self.txt[item])\n",
    "        author = self.authors[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            txt,\n",
    "            \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "\n",
    "        )\n",
    "        return {\n",
    "            'txt':txt,\n",
    "            'input_id':encoding['input_ids'].flatten(),\n",
    "            'attention_mask':encoding['attention_mask'].flatten(),\n",
    "            'authors': torch.tensor(author,dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val = train_test_split(df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Dataloader\n",
    "\n",
    "def create_dataloader(df, batch_size,max_len, num_workers,shuffle=True):\n",
    "    ds = auth_dataset(\n",
    "        txt = df.text.to_numpy(),\n",
    "        authors=df.author_encoder.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "Num_workers = 0\n",
    "train_data_loader = create_dataloader(df_train, BATCH_SIZE,MAX_LEN, Num_workers, shuffle=True)\n",
    "val_data_loader = create_dataloader(df_val, BATCH_SIZE,MAX_LEN, Num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['txt', 'input_id', 'attention_mask', 'authors'])\n",
      "tensor([[  101,  1109, 13874,  ...,     0,     0,     0],\n",
      "        [  101,  1124,  1108,  ...,     0,     0,     0],\n",
      "        [  101, 13197,  1122,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 25893,  1181,  ...,     0,     0,     0],\n",
      "        [  101,   107, 10779,  ...,     0,     0,     0],\n",
      "        [  101,  1109,  2915,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "\n",
    "# print(data['txt'])\n",
    "print(data['input_id'])\n",
    "print(data['attention_mask'])\n",
    "print(data['authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentiment Classifier\n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_classes):\n",
    "        super(SentimentClassifier,self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size,n_classes)\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        pooled_output = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )[1] \n",
    "\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "unique_authors = df['author'].unique()\n",
    "print(unique_authors)\n",
    "\n",
    "\n",
    "model = SentimentClassifier(len(unique_authors))\n",
    "model = model.to(device)\n",
    "\n",
    "# Number of hidden units\n",
    "print(bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch=10\n",
    "optim = AdamW(model.parameters(),lr=2e-5,correct_bias=False)\n",
    "\n",
    "total_step = len(train_data_loader)*Epoch\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_step)\n",
    "\n",
    "loss_fn= nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,data_loader,loss_fn,optim,device,scheduler,n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_pred =0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_id\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        authors = d['authors'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "\n",
    "        _,pred = torch.max(outputs,dim=1)\n",
    "        loss = loss_fn(outputs,authors)\n",
    "        correct_pred += torch.sum(pred == authors)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        optim.zero_grad() \n",
    "        \n",
    "\n",
    "    return correct_pred.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,data_loader, loss_fn,device,n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_pred = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_id\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            authors = d['authors'].to(device)\n",
    "\n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, authors)\n",
    "            \n",
    "            correct_pred += torch.sum(preds == authors)\n",
    "            losses.append(loss.item())\n",
    "    return correct_pred.double()/n_examples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "history = defaultdict(list)\n",
    "for epoch in range(Epoch):\n",
    "    \n",
    "    # Show details \n",
    "    print(f\"Epoch {epoch + 1}/{Epoch}\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optim,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "    \n",
    "    # Get model performance (accuracy and loss)\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # If we beat prev performance\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "\n",
    "# Extract scalar values from the tensors and convert to numpy arrays\n",
    "train_acc_np = np.array([item.item() for item in history['train_acc']])\n",
    "val_acc_np = np.array([item.item() for item in history['val_acc']])\n",
    "\n",
    "\n",
    "plt.plot(train_acc_np, label='train accuracy')\n",
    "plt.plot(val_acc_np, label='validation accuracy')\n",
    "\n",
    "# Graph chars\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a helper function to get predictions from our models. This is similar to the evaluation function, except that we’re storing the text of the reviews and the predicted probabilities\n",
    "\n",
    "def get_pred(model,data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    txts =[]\n",
    "    pred = []\n",
    "    pred_prob = []\n",
    "    real_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_id\"].to(device)\n",
    "            txt  = d[\"txt\"]\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            authors = d['authors'].to(device)\n",
    "\n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            txts.extend(txt)\n",
    "            pred.extend(preds)\n",
    "            pred_prob.extend(outputs)\n",
    "            real_val.extend(authors)\n",
    "\n",
    "    pred = torch.stack(pred).cpu()\n",
    "    pred_prob = torch.stack(pred_prob).cpu()\n",
    "    real_val = torch.stack(real_val).cpu()\n",
    "\n",
    "    return txts,pred,pred_prob,real_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_review_texts, y_pred, y_pred_probs, y_test = get_pred(\n",
    "    model,\n",
    "    val_data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=unique_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=unique_authors, columns=unique_authors)\n",
    "show_confusion_matrix(df_cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict raw text\n",
    "review_text = 'It never once occurred to me that the fumbling might be a mere mistake.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_view = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length = MAX_LEN,\n",
    "    add_special_tokens = True,\n",
    "    return_token_type_ids = False,\n",
    "    pad_to_max_length = True,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = 'pt',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_view['input_ids'].to(device)\n",
    "attention_mask = encoded_view['attention_mask'].to(device)\n",
    "\n",
    "output = model (input_ids,attention_mask)\n",
    "_, pred = torch.max(output,dim=1)\n",
    "\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {unique_authors[pred]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Bert on Infernce Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unseen_auth_dataset(Dataset):\n",
    "    def __init__(self, txt, id, tokenizer, max_len):\n",
    "        self.txt = txt\n",
    "        self.id = id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.txt)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        txt = str(self.txt[item])\n",
    "        id_ = self.id[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            txt,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'id': id_,\n",
    "            'txt': txt,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unseen_dataloader(df, batch_size, max_len, num_workers, shuffle=True):\n",
    "    ds = unseen_auth_dataset(\n",
    "        txt=df.text.to_numpy(),\n",
    "        id=df.id.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_dataset = create_unseen_dataloader(test, BATCH_SIZE, MAX_LEN, Num_workers, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = next(iter(unseen_dataset))\n",
    "data_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_data_get_pred(model,data_loader):\n",
    "\n",
    "    model = model.eval()\n",
    "    ids = []\n",
    "    txts =[]\n",
    "    pred = []\n",
    "    pred_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            txt  = d[\"txt\"]\n",
    "            id = d[\"id\"]\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            ids.extend(id)\n",
    "            txts.extend(txt)\n",
    "            pred.extend(preds)\n",
    "            pred_prob.extend(outputs)\n",
    "            \n",
    "\n",
    "    pred = torch.stack(pred).cpu()\n",
    "    pred_prob = torch.stack(pred_prob).cpu()\n",
    "\n",
    "    return txts,pred,pred_prob,ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_ids = unseen_data_get_pred(\n",
    "    model,\n",
    "    unseen_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id       EAP       HPL       MWS\n",
      "0     id06617  8.333536 -3.732450 -3.290669\n",
      "1     id22789 -4.320556 -4.206328  8.130929\n",
      "2     id15726 -4.315736 -4.263460  8.090265\n",
      "3     id19077 -4.007131  8.218512 -3.625121\n",
      "4     id27537  8.471301 -4.089209 -3.262084\n",
      "...       ...       ...       ...       ...\n",
      "8387  id00335  8.347522 -4.157529 -2.928862\n",
      "8388  id07054 -4.042836 -4.528657  7.536205\n",
      "8389  id19578  8.526666 -3.780602 -3.477422\n",
      "8390  id22422  8.453132 -3.574438 -3.551195\n",
      "8391  id15215 -4.006074  8.351609 -3.855753\n",
      "\n",
      "[8392 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    'id': y_ids,\n",
    "    'EAP': y_pred_probs[:, 0],  # Assuming the first column corresponds to class 'EAP'\n",
    "    'HPL': y_pred_probs[:, 1],  # Assuming the second column corresponds to class 'HPL'\n",
    "    'MWS': y_pred_probs[:, 2]   # Assuming the third column corresponds to class 'MWS'\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
